{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# 对大数据进行预处理\n",
    "\n",
    "以占领华尔街推特数据为例\n",
    "\n",
    "\n",
    "\n",
    "![image.png](./images/author.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 字节（Byte /bait/）\n",
    "\n",
    "计算机信息技术用于计量存储容量的一种计量单位，通常情况下一字节等于有八位， [1]  也表示一些计算机编程语言中的数据类型和语言字符。\n",
    "- 1B（byte，字节）= 8 bit；\n",
    "- 1KB=1000B；1MB=1000KB=1000×1000B。其中1000=10^3。\n",
    "- 1KB（kilobyte，千字节）=1000B= 10^3 B；\n",
    "- 1MB（Megabyte，兆字节，百万字节，简称“兆”）=1000KB= 10^6 B；\n",
    "- 1GB（Gigabyte，吉字节，十亿字节，又称“千兆”）=1000MB= 10^9 B；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 按照Chunk读取数据并进行处理\n",
    "\n",
    "Lazy Method for Reading Big File in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T01:33:49.310206Z",
     "start_time": "2020-10-20T01:33:49.296288Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2754\n"
     ]
    }
   ],
   "source": [
    "bigfile = open('/Users/datalab/bigdata/cjc/ows-raw.txt', 'r')\n",
    "chunkSize = 1000000\n",
    "chunk = bigfile.readlines(chunkSize)\n",
    "print(len(chunk))\n",
    "# with open(\"../data/ows_tweets_sample.txt\", 'w') as f:\n",
    "#     for i in chunk:\n",
    "#         f.write(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T01:48:30.428016Z",
     "start_time": "2020-10-20T01:48:30.363808Z"
    }
   },
   "outputs": [],
   "source": [
    "bigfile.readlines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T01:50:03.152797Z",
     "start_time": "2020-10-20T01:50:03.149044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5%5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T01:51:04.500263Z",
     "start_time": "2020-10-20T01:50:11.366522Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 262665\n",
      "5 1574666\n",
      "10 2880857\n",
      "15 4189419\n",
      "20 5492578\n",
      "25 6602141\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python?lq=1\n",
    "import csv\n",
    "bigfile = open('/Users/datalab/bigdata/cjc/ows-raw.txt', 'r')\n",
    "chunkSize = 10**8\n",
    "chunk = bigfile.readlines(chunkSize)\n",
    "num_chunk, num_lines = 0, 0\n",
    "while chunk:\n",
    "    lines = csv.reader((line.replace('\\x00','') for line in chunk), \n",
    "                       delimiter=',', quotechar='\"')\n",
    "    #do sth.\n",
    "    num_lines += len(list(lines))\n",
    "    if num_chunk % 5 ==0:\n",
    "        print(num_chunk, num_lines)\n",
    "    num_chunk += 1\n",
    "    chunk = bigfile.readlines(chunkSize) # read another chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T01:47:05.242288Z",
     "start_time": "2020-10-20T01:47:05.233335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6602141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 用Pandas的get_chunk功能来处理亿级数据\n",
    "\n",
    "> 只有在超过5TB数据量的规模下，Hadoop才是一个合理的技术选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T08:39:32.629430Z",
     "start_time": "2020-06-06T08:39:31.503242Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f = open('/Users/datalab/bigdata/cjc/ows-raw.txt',encoding='utf-8')\n",
    "reader = pd.read_table(f,  sep=',',  quotechar='\"', iterator=True, error_bad_lines=False) #跳过报错行\n",
    "chunkSize = 100000\n",
    "chunk = reader.get_chunk(chunkSize)\n",
    "len(chunk)\n",
    "\n",
    "#pd.read_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T08:39:55.945174Z",
     "start_time": "2020-06-06T08:39:55.896742Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Twitter ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Profile Image URL</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Geo</th>\n",
       "      <th>From User</th>\n",
       "      <th>From User ID</th>\n",
       "      <th>Language</th>\n",
       "      <th>To User</th>\n",
       "      <th>To User ID</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121813144174727168</td>\n",
       "      <td>RT @AnonKitsu: ALERT!!!!!!!!!!COPS ARE KETTLIN...</td>\n",
       "      <td>http://a2.twimg.com/profile_images/1539375713/...</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-10-06 05:04:51</td>\n",
       "      <td>N;</td>\n",
       "      <td>Anonops_Cop</td>\n",
       "      <td>401240477</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;a href=&amp;quot;http://twitter.com/&amp;quot;&amp;gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121813146137657344</td>\n",
       "      <td>@jamiekilstein @allisonkilkenny Interesting in...</td>\n",
       "      <td>http://a2.twimg.com/profile_images/1574715503/...</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-10-06 05:04:51</td>\n",
       "      <td>N;</td>\n",
       "      <td>KittyHybrid</td>\n",
       "      <td>34532053</td>\n",
       "      <td>en</td>\n",
       "      <td>jamiekilstein</td>\n",
       "      <td>2149053</td>\n",
       "      <td>&amp;lt;a href=&amp;quot;http://twitter.com/&amp;quot;&amp;gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121813150000619521</td>\n",
       "      <td>@Seductivpancake Right! Those guys have a vict...</td>\n",
       "      <td>http://a1.twimg.com/profile_images/1241412831/...</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-10-06 05:04:52</td>\n",
       "      <td>N;</td>\n",
       "      <td>nerdsherpa</td>\n",
       "      <td>95067344</td>\n",
       "      <td>en</td>\n",
       "      <td>Seductivpancake</td>\n",
       "      <td>19695580</td>\n",
       "      <td>&amp;lt;a href=&amp;quot;http://www.echofon.com/&amp;quot;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121813150701072385</td>\n",
       "      <td>RT @bembel &amp;quot;Occupy Wall Street&amp;quot; als ...</td>\n",
       "      <td>http://a0.twimg.com/profile_images/1106399092/...</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-10-06 05:04:52</td>\n",
       "      <td>N;</td>\n",
       "      <td>hamudistan</td>\n",
       "      <td>35862923</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;a href=&amp;quot;http://levelupstudio.com&amp;quot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121813163778899968</td>\n",
       "      <td>#ows White shirt= Brown shirt.</td>\n",
       "      <td>http://a2.twimg.com/profile_images/1568117871/...</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-10-06 05:04:56</td>\n",
       "      <td>N;</td>\n",
       "      <td>kl_knox</td>\n",
       "      <td>419580636</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;a href=&amp;quot;http://twitter.com/&amp;quot;&amp;gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Twitter ID                                               Text  \\\n",
       "0  121813144174727168  RT @AnonKitsu: ALERT!!!!!!!!!!COPS ARE KETTLIN...   \n",
       "1  121813146137657344  @jamiekilstein @allisonkilkenny Interesting in...   \n",
       "2  121813150000619521  @Seductivpancake Right! Those guys have a vict...   \n",
       "3  121813150701072385  RT @bembel &quot;Occupy Wall Street&quot; als ...   \n",
       "4  121813163778899968                     #ows White shirt= Brown shirt.   \n",
       "\n",
       "                                   Profile Image URL         Day  Hour  \\\n",
       "0  http://a2.twimg.com/profile_images/1539375713/...  2011-10-06     5   \n",
       "1  http://a2.twimg.com/profile_images/1574715503/...  2011-10-06     5   \n",
       "2  http://a1.twimg.com/profile_images/1241412831/...  2011-10-06     5   \n",
       "3  http://a0.twimg.com/profile_images/1106399092/...  2011-10-06     5   \n",
       "4  http://a2.twimg.com/profile_images/1568117871/...  2011-10-06     5   \n",
       "\n",
       "   Minute           Created At Geo    From User  From User ID Language  \\\n",
       "0       4  2011-10-06 05:04:51  N;  Anonops_Cop     401240477       en   \n",
       "1       4  2011-10-06 05:04:51  N;  KittyHybrid      34532053       en   \n",
       "2       4  2011-10-06 05:04:52  N;   nerdsherpa      95067344       en   \n",
       "3       4  2011-10-06 05:04:52  N;   hamudistan      35862923       en   \n",
       "4       4  2011-10-06 05:04:56  N;      kl_knox     419580636       en   \n",
       "\n",
       "           To User  To User ID  \\\n",
       "0              NaN           0   \n",
       "1    jamiekilstein     2149053   \n",
       "2  Seductivpancake    19695580   \n",
       "3              NaN           0   \n",
       "4              NaN           0   \n",
       "\n",
       "                                              Source  \n",
       "0  &lt;a href=&quot;http://twitter.com/&quot;&gt;...  \n",
       "1  &lt;a href=&quot;http://twitter.com/&quot;&gt;...  \n",
       "2  &lt;a href=&quot;http://www.echofon.com/&quot;...  \n",
       "3  &lt;a href=&quot;http://levelupstudio.com&quot...  \n",
       "4  &lt;a href=&quot;http://twitter.com/&quot;&gt;...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T01:45:00.835168Z",
     "start_time": "2020-10-20T01:44:25.200769Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100000\n",
      "1 200000\n",
      "2 300000\n",
      "3 400000\n",
      "4 500000\n",
      "5 600000\n",
      "6 700000\n",
      "7 800000\n",
      "8 900000\n",
      "9 1000000\n",
      "10 1100000\n",
      "11 1200000\n",
      "12 1300000\n",
      "13 1400000\n",
      "14 1500000\n",
      "15 1600000\n",
      "16 1700000\n",
      "17 1800000\n",
      "18 1900000\n",
      "19 2000000\n",
      "20 2100000\n",
      "21 2200000\n",
      "22 2300000\n",
      "23 2400000\n",
      "24 2500000\n",
      "25 2600000\n",
      "26 2700000\n",
      "27 2800000\n",
      "28 2900000\n",
      "29 3000000\n",
      "30 3100000\n",
      "31 3200000\n",
      "32 3300000\n",
      "33 3400000\n",
      "34 3500000\n",
      "35 3600000\n",
      "36 3700000\n",
      "37 3800000\n",
      "38 3900000\n",
      "39 4000000\n",
      "40 4100000\n",
      "41 4200000\n",
      "42 4300000\n",
      "43 4400000\n",
      "44 4500000\n",
      "45 4600000\n",
      "46 4700000\n",
      "47 4800000\n",
      "48 4900000\n",
      "49 5000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5051743: expected 14 fields, saw 15\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 5100000\n",
      "51 5200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5254718: expected 14 fields, saw 15\\n'\n",
      "b'Skipping line 5281095: expected 14 fields, saw 15\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 5300000\n",
      "53 5400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5481759: expected 14 fields, saw 15\\nSkipping line 5482014: expected 14 fields, saw 15\\nSkipping line 5482532: expected 14 fields, saw 15\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 5500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5516605: expected 14 fields, saw 15\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 5600000\n",
      "56 5700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5709055: expected 14 fields, saw 15\\n'\n",
      "b'Skipping line 5796658: expected 14 fields, saw 15\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 5800000\n",
      "58 5900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5927412: expected 14 fields, saw 15\\nSkipping line 5927419: expected 14 fields, saw 15\\nSkipping line 5927421: expected 14 fields, saw 15\\nSkipping line 5927451: expected 14 fields, saw 15\\nSkipping line 5927478: expected 14 fields, saw 15\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 6000000\n",
      "60 6100000\n",
      "61 6200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6229621: expected 14 fields, saw 16\\nSkipping line 6245861: expected 14 fields, saw 17\\n'\n",
      "b'Skipping line 6278728: expected 14 fields, saw 15\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 6300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6350262: expected 14 fields, saw 15\\n'\n",
      "b'Skipping line 6387321: expected 14 fields, saw 15\\nSkipping line 6388879: expected 14 fields, saw 15\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 6400000\n",
      "64 6500000\n",
      "65 6600000\n",
      "66 6602120\n",
      "Iteration is stopped.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f = open('/Users/datalab/bigdata/cjc/ows-raw.txt',encoding='utf-8')\n",
    "reader = pd.read_table(f,  sep=',',  quotechar='\"', \n",
    "                       iterator=True, error_bad_lines=False) #跳过报错行\n",
    "chunkSize = 100000\n",
    "loop = True\n",
    "#data = []\n",
    "num_chunk, num_lines = 0, 0\n",
    "while loop:\n",
    "    try:\n",
    "        chunk = reader.get_chunk(chunkSize)\n",
    "        # dat = data_cleaning_funtion(chunk) # do sth.\n",
    "        num_lines += len(chunk)\n",
    "        print(num_chunk, num_lines)\n",
    "        num_chunk +=1\n",
    "        #data.append(dat) \n",
    "    except StopIteration:\n",
    "        loop = False\n",
    "        print(\"Iteration is stopped.\")\n",
    "#df = pd.concat(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](./images/end.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1260px",
    "left": "1835px",
    "top": "224px",
    "width": "512px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
